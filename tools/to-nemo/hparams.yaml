cfg:
  micro_batch_size: 64
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  encoder_seq_length: 512
  max_position_embeddings: 512
  num_layers: 12
  hidden_size: 768
  ffn_hidden_size: 3072
  num_attention_heads: 12
  init_method_std: 0.02
  hidden_dropout: 0.1
  kv_channels: null
  apply_query_key_layer_scaling: true
  layernorm_epsilon: 1.0e-05
  make_vocab_size_divisible_by: 128
  pre_process: true
  post_process: true
  bert_binary_head: true
  tokenizer:
    library: megatron
    type: BertWordPieceLowerCase
    model: null
    vocab_file: /home/jqi/work/megatron/vocab/jq/jq-tokens.txt.2.vocab
   #vocab_file: /mount/megatron/Megatron-LM/vocab/jq/jq-tokens.txt.2.vocab
    merge_file: null
    tokenizer_name: bert-base-uncased
    special_tokens: ''
    tokenizer_model: null
  native_amp_init_scale: 4294967296
  native_amp_growth_interval: 1000
  fp32_residual_connection: false
  fp16_lm_cross_entropy: false
  seed: 1234
  use_cpu_initialization: false
  onnx_safe: false
  activations_checkpoint_method: null
  activations_checkpoint_num_layers: 1
  data:
    data_prefix: ''
    index_mapping_dir: null
    data_impl: mmap
    splits_string: 950,49,1
    seq_length: 128
    skip_warmup: true
    num_workers: 0
    dataloader_type: single
    reset_position_ids: false
    reset_attention_mask: false
    eod_mask_loss: false
    masked_lm_prob: 0.15
    short_seq_prob: 0.1
  optim:
    name: fused_adam
    lr: 0.001
    weight_decay: 0.01
    betas:
    - 0.9
    - 0.98
    sched:
      name: CosineAnnealing
      warmup_steps: 500
      constant_steps: 50000
      min_lr: 2.0e-05
  megatron_legacy: false
  bias_gelu_fusion: true
  masked_softmax_fusion: true
